{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, hamming_loss, f1_score\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_JOB_ID         object\n",
      "DESCRIPTION            object\n",
      "BRAND_NAME             object\n",
      "MODEL_NUMBER           object\n",
      "PROBLEM_DESCRIPTION    object\n",
      "SERVICE_EXPLANATION    object\n",
      "PART_NUMBER_ORDERED    object\n",
      "PART_DESCRIPTION       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '/Users/key.houck/Desktop/parts-ml-poc/data/at_service_job_AT_SJ_PART_AT_SO_PRODUCT_AT_SA_SERVICE_JOB_PRODUC_202402141028.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace '[NULL]' with NaN and then drop rows with any NaN values\n",
    "data = data.replace('[NULL]', pd.NA).dropna()\n",
    "\n",
    "# TODO: Only filter for SAMSUNG ELECTRONICS in BRAND_NAME\n",
    "data = data[data['BRAND_NAME'] == 'SAMSUNG ELECTRONICS']\n",
    "\n",
    "# Since the 'SERVICE_JOB_ID' will not be used as a feature, we'll exclude it from the feature set\n",
    "data_features = data.drop(columns=['SERVICE_JOB_ID', 'PART_NUMBER_ORDERED', 'PART_DESCRIPTION', 'SERVICE_EXPLANATION'])\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode, Vectorize, and multi-label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the PROBLEM_DESCRIPTION text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "data['PROBLEM_DESCRIPTION_CLEANED'] = data['PROBLEM_DESCRIPTION'].apply(clean_text)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for column in ['DESCRIPTION', 'BRAND_NAME', 'MODEL_NUMBER']:\n",
    "    le = LabelEncoder()\n",
    "    data[column + '_ENCODED'] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Vectorize the cleaned PROBLEM_DESCRIPTION\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "problem_description_tfidf = tfidf_vectorizer.fit_transform(data['PROBLEM_DESCRIPTION_CLEANED'])\n",
    "\n",
    "# Combine encoded categorical features and TF-IDF vectorized text into one feature set\n",
    "categorical_features = ['DESCRIPTION_ENCODED', 'BRAND_NAME_ENCODED', 'MODEL_NUMBER_ENCODED']\n",
    "encoded_categorical = data[categorical_features].values\n",
    "all_features = hstack([encoded_categorical, problem_description_tfidf])\n",
    "\n",
    "# Prepare the target for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "multi_label_target = mlb.fit_transform(data['PART_NUMBER_ORDERED'].apply(lambda x: set(x.split(','))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, multi_label_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MultiOutputClassifier with RandomForestClassifier as the base estimator\n",
    "model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.00019622074488433974\n",
      "F1 Score (Micro): 0.10494425593795444\n",
      "F1 Score (Samples): 0.0712171052631579\n",
      "Subset Accuracy (Exact Match): 0.0712171052631579\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Hamming Loss: Lower values are better, indicating fewer incorrect labels.\n",
    "hamming_loss_value = hamming_loss(y_test, y_pred)\n",
    "print(f\"Hamming Loss: {hamming_loss_value}\")\n",
    "\n",
    "# F1 Score: Harmonic mean of precision and recall. A higher score indicates better performance.\n",
    "# Calculating for each label ('micro') gives equal weight to all labels.\n",
    "f1_score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(f\"F1 Score (Micro): {f1_score_micro}\")\n",
    "\n",
    "# Calculating for each instance ('samples') and then averaging is also useful.\n",
    "f1_score_samples = f1_score(y_test, y_pred, average='samples')\n",
    "print(f\"F1 Score (Samples): {f1_score_samples}\")\n",
    "\n",
    "# Subset Accuracy: Exact match score you've already computed. Reiterating for context.\n",
    "subset_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DESCRIPTION': LabelEncoder(), 'BRAND_NAME': LabelEncoder(), 'MODEL_NUMBER': LabelEncoder()}\n",
      "[()]\n"
     ]
    }
   ],
   "source": [
    "def predict_parts(description, brand_name, model_number, problem_description):\n",
    "    # Clean the problem description text\n",
    "    cleaned_problem_description = clean_text(problem_description)  # Assuming `clean_text` is your text cleaning function\n",
    "    \n",
    "    print(label_encoders)\n",
    "\n",
    "    # Encode categorical features\n",
    "    description_encoded = label_encoders['DESCRIPTION'].transform([description])  # Assuming le_description is your fitted LabelEncoder for DESCRIPTION\n",
    "    brand_name_encoded = label_encoders['BRAND_NAME'].transform([brand_name])  # Assuming le_brand_name is your fitted LabelEncoder for BRAND_NAME\n",
    "    model_number_encoded = label_encoders['MODEL_NUMBER'].transform([model_number])  # Assuming le_model_number is your fitted LabelEncoder for MODEL_NUMBER\n",
    "    \n",
    "    # Vectorize the problem description\n",
    "    problem_description_vectorized = tfidf_vectorizer.transform([cleaned_problem_description])  # tfidf_vectorizer is your fitted TfidfVectorizer\n",
    "    \n",
    "    # Combine all features into a single feature vector\n",
    "    features = hstack([description_encoded.reshape(-1, 1), brand_name_encoded.reshape(-1, 1), model_number_encoded.reshape(-1, 1), problem_description_vectorized])\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    # Convert the prediction back to the list of parts\n",
    "    predicted_parts = mlb.inverse_transform(prediction)  # mlb is your fitted MultiLabelBinarizer\n",
    "    \n",
    "    return predicted_parts\n",
    "\n",
    "# Example usage\n",
    "predicted_parts = predict_parts('HOME REFRIGERATION', 'SAMSUNG ELECTRONICS', 'RF28HMEDBSR/AA', 'Ice tray is leaking')\n",
    "print(predicted_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
